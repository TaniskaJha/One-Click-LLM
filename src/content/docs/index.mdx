---
title: Deploy LLM Inference APIs anywhere within seconds
description: Deploy any open source Large Language models from Huggingface with zero configuration
template: splash
hero:
  tagline: Deploy any open source Large Language model (LLM) from Huggingface or Ollama Registry to Akash network with zero configuration
  image:
    file: ../../assets/ai.svg
  actions:
    - text: Deploy your own LLM
      link: /deploy-you-own-llm
      icon: right-arrow
      variant: primary
    - text: Free Demo API
      link: /free-demo-api/example/
      icon: external
---

import { Card, CardGrid } from "@astrojs/starlight/components";

## Streamling LLM Infrence API Deployment on any cloud plaform via Ollama, vLLM and Llama-cpp

Deploy open source models within seconds on Akash Network:

1. Select the Model of your choice from Huggingface or Ollama registry.
2. Hit Deploy to get a properly configured compose file with infernce api of the selected model.
3. Copy the compose and use it to deploy on GCP, Azure or AWS.

#### The API is fully compatible with the popular openAI library and it acts as a drop in replacement. 

The deployment process is fully automated and managed and needs no configuration, which ensures that the service is highly available, scalable, and secure.

-------------------------------------------------------------
<CardGrid stagger>

  <Card  title="Free Demo API" icon="open-book">
  [Click here](/free-demo-api/example/)  to use our free API
  </Card>
  <Card  title="Endpoint Documentation" icon="open-book">
  [Click here](guides/example/) to see the user docs
  </Card>
</CardGrid>
